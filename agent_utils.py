# -*- coding: utf-8 -*-
"""agent_utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BYvd-caEpZHVuRmWeVJkPTBvlxFEc5XD
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile agent_utils.py
# # --- agent_utils.py ---
# import re
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# 
# from sqlalchemy import create_engine, inspect
# from sentence_transformers import SentenceTransformer
# import faiss
# 
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# 
# 
# # -----------------------------
# # DB (points to your local sqlite file)
# # -----------------------------
# ENGINE = create_engine("sqlite:///ecommerce.db")
# 
# 
# # -----------------------------
# # Load models (cache in Streamlit later)
# # -----------------------------
# SQL_MODEL_NAME = "defog/sqlcoder-7b-2"
# EXPLAIN_MODEL_NAME = "microsoft/Phi-3.5-mini-instruct"
# EMBED_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
# 
# tokenizer = AutoTokenizer.from_pretrained(SQL_MODEL_NAME)
# model = AutoModelForCausalLM.from_pretrained(
#     SQL_MODEL_NAME,
#     device_map="auto",
#     torch_dtype="auto"
# )
# 
# explainer = pipeline(
#     "text-generation",
#     model=EXPLAIN_MODEL_NAME,
#     max_new_tokens=200
# )
# 
# embedder = SentenceTransformer(EMBED_MODEL_NAME)
# 
# 
# # -----------------------------
# # Schema RAG build (once)
# # -----------------------------
# def build_schema_rag():
#     inspector = inspect(ENGINE)
#     tables = inspector.get_table_names()
# 
#     schema_chunks = []
#     for table in tables:
#         cols = inspector.get_columns(table)
#         lines = [f"- {c['name']} ({str(c['type'])})" for c in cols]
#         schema_chunks.append(f"Table: {table}\nColumns:\n" + "\n".join(lines))
# 
#     relationships_chunk = """
# Relationships:
# - orders.customer_id → customers.customer_id
# - orders.product_id → products.product_id
# 
# SQLite Date Rules:
# - No date_trunc or EXTRACT
# - Use strftime('%Y-%m', order_date) for month grouping
# """
#     schema_chunks.append(relationships_chunk)
# 
#     embeddings = embedder.encode(schema_chunks, normalize_embeddings=True)
#     index = faiss.IndexFlatIP(embeddings.shape[1])
#     index.add(embeddings.astype(np.float32))
# 
#     return schema_chunks, index, relationships_chunk
# 
# 
# SCHEMA_CHUNKS, FAISS_INDEX, REL_CHUNK = build_schema_rag()
# 
# 
# def retrieve_schema(question: str, top_k: int = 3) -> str:
#     q_emb = embedder.encode([question], normalize_embeddings=True).astype(np.float32)
#     _, ids = FAISS_INDEX.search(q_emb, top_k)
#     retrieved = [SCHEMA_CHUNKS[i] for i in ids[0]]
# 
#     if REL_CHUNK not in retrieved:
#         retrieved.append(REL_CHUNK)
# 
#     return "\n\n".join(retrieved)
# 
# 
# # -----------------------------
# # SQL helpers
# # -----------------------------
# def extract_sql(text: str) -> str:
#     text = text.strip()
#     text = re.sub(r"^```(?:sql)?\s*", "", text, flags=re.IGNORECASE)
#     text = re.sub(r"\s*```$", "", text)
# 
#     if "SQL:" in text:
#         text = text.split("SQL:")[-1].strip()
# 
#     m = re.search(r"(SELECT\s.+)", text, flags=re.IGNORECASE | re.DOTALL)
#     return m.group(1).strip() if m else text.strip()
# 
# 
# def is_safe_sql(sql: str) -> bool:
#     forbidden = ["INSERT", "UPDATE", "DELETE", "DROP", "ALTER", "TRUNCATE", "CREATE"]
#     s = sql.upper()
#     return not any(w in s for w in forbidden)
# 
# 
# def run_sql(sql: str) -> pd.DataFrame:
#     if not is_safe_sql(sql):
#         raise ValueError("Unsafe SQL detected")
#     return pd.read_sql(sql, ENGINE)
# 
# 
# # -----------------------------
# # Prompts
# # -----------------------------
# def build_sql_prompt(question: str, schema_text: str, memory_context: str = "") -> str:
#     return f"""
# You are a senior data analyst writing SQLite SQL.
# 
# IMPORTANT SQLITE RULES:
# - SQLite does NOT support date_trunc or EXTRACT.
# - For month grouping use strftime('%Y-%m', date_column).
# 
# If memory context exists, treat question as a follow-up and modify previous SQL.
# 
# Rules:
# - SELECT only
# - Use only provided schema
# - Use relationships for joins
# - Return ONLY SQL
# 
# Schema:
# {schema_text}
# 
# {memory_context}
# 
# Question:
# {question}
# 
# SQL:
# """.strip()
# 
# 
# def build_sql_repair_prompt(question: str, schema_text: str, bad_sql: str, error_msg: str) -> str:
#     return f"""
# You are an expert SQLite SQL developer.
# 
# IMPORTANT SQLITE RULES:
# - No date_trunc or EXTRACT.
# - Use strftime('%Y-%m', date_column) for month grouping.
# 
# Fix the SQL based on the error.
# Return ONE corrected SQLite SELECT query only.
# 
# Schema:
# {schema_text}
# 
# Question:
# {question}
# 
# Failed SQL:
# {bad_sql}
# 
# SQLite Error:
# {error_msg}
# 
# Corrected SQL:
# """.strip()
# 
# 
# # -----------------------------
# # Generation + Repair loop
# # -----------------------------
# def generate_sql(question: str, memory_context: str = "", top_k: int = 3) -> str:
#     schema_text = retrieve_schema(question, top_k=top_k)
#     prompt = build_sql_prompt(question, schema_text, memory_context)
# 
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#     outputs = model.generate(**inputs, max_new_tokens=256, do_sample=False, temperature=0)
#     text = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return extract_sql(text)
# 
# 
# def repair_sql(question: str, bad_sql: str, error_msg: str, top_k: int = 3) -> str:
#     schema_text = retrieve_schema(question, top_k=top_k)
#     prompt = build_sql_repair_prompt(question, schema_text, bad_sql, error_msg)
# 
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#     outputs = model.generate(**inputs, max_new_tokens=256, do_sample=False, temperature=0)
#     text = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return extract_sql(text)
# 
# 
# def run_sql_with_autorepair(question: str, initial_sql: str, max_retries: int = 2, top_k: int = 3):
#     sql = extract_sql(initial_sql)
#     last_error = None
# 
#     for attempt in range(max_retries + 1):
#         if not is_safe_sql(sql):
#             raise ValueError("Unsafe SQL detected")
# 
#         try:
#             df = run_sql(sql)
#             return sql, df
#         except Exception as e:
#             last_error = str(e)
#             if attempt == max_retries:
#                 break
#             sql = repair_sql(question, sql, last_error, top_k=top_k)
# 
#     raise RuntimeError(f"Failed after retries. Last error: {last_error}")
# 
# 
# # -----------------------------
# # Explanation
# # -----------------------------
# def explain_results(question: str, sql: str, df: pd.DataFrame) -> str:
#     prompt = f"""
# You are a senior data analyst.
# 
# Question:
# {question}
# 
# SQL:
# {sql}
# 
# Result:
# {df.head(10).to_string(index=False)}
# 
# Explain insights in simple business terms.
# """.strip()
# 
#     return explainer(prompt)[0]["generated_text"]
# 
# 
# # -----------------------------
# # Auto chart (matplotlib)
# # -----------------------------
# def auto_plot(df: pd.DataFrame, title: str):
#     if df is None or df.empty or df.shape[1] < 2:
#         return None
# 
#     numeric_cols = df.select_dtypes(include="number").columns.tolist()
#     non_numeric_cols = df.select_dtypes(exclude="number").columns.tolist()
#     if not numeric_cols or not non_numeric_cols:
#         return None
# 
#     dim = non_numeric_cols[0]
#     metric = numeric_cols[0]
# 
#     fig = plt.figure(figsize=(10, 5))
#     if df[dim].astype(str).str.match(r"\d{4}-\d{2}").any():
#         plt.plot(df[dim].astype(str), df[metric])
#         plt.xticks(rotation=45)
#     else:
#         plt.bar(df[dim].astype(str), df[metric])
#         plt.xticks(rotation=45)
# 
#     plt.title(title)
#     plt.xlabel(dim)
#     plt.ylabel(metric)
#     plt.tight_layout()
#     return fig
#